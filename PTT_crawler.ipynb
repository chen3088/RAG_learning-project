{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3fc6bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Functions\n",
    "def crawl_posts(num_pages=5):\n",
    "    \"\"\"Main function to crawl posts from PTT\"\"\"\n",
    "    latest_index = get_latest_index()\n",
    "    if not latest_index:\n",
    "        print(\"âŒ ç„¡æ³•å–å¾—æœ€æ–°é ç¢¼\")\n",
    "        return None\n",
    "        \n",
    "    all_articles = []\n",
    "    for page_index in tqdm(range(latest_index, latest_index - num_pages, -1)):\n",
    "        try:\n",
    "            articles = crawl_page(page_index)\n",
    "            all_articles.extend(articles)\n",
    "            random_sleep(CONFIG['PAGE_SLEEP_TIME_RANGE'])\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ç™¼ç”ŸéŒ¯èª¤æ–¼ index{page_index}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(all_articles)\n",
    "\n",
    "def process_and_save_data(df, threshold=None, filename=None):\n",
    "    \"\"\"Process and save filtered data\"\"\"\n",
    "    threshold = threshold or CONFIG['DEFAULT_THRESHOLD']\n",
    "    if filename is None:\n",
    "        filename = f'{CONFIG[\"BOARD_NAME\"]}_above_{threshold}_rec.csv'  # Changed from 'below' to 'above'\n",
    "    \n",
    "    # Filter and save data\n",
    "    filtered_df = process_recommendations(df, threshold)\n",
    "    output_path = get_data_path(filename)\n",
    "    filtered_df.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "    \n",
    "    # Print status\n",
    "    print(f\"âœ… Successfully saved {len(filtered_df)} records with {threshold}+ recommendations to {output_path}\")\n",
    "    print(f\"ðŸ“ File size: {os.path.getsize(output_path) / 1024:.1f} KB\")\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e830c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def visualize_recommendations(filtered_df, threshold=None):\n",
    "    \"\"\"Visualize the distribution of recommendations\"\"\"\n",
    "    threshold = threshold or CONFIG['DEFAULT_THRESHOLD']\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"Posts with {threshold} or more recommendations:\")\n",
    "    print(f\"Total count: {len(filtered_df)}\")\n",
    "    print(\"\\nFiltered data preview:\")\n",
    "    print(filtered_df[['title', 'nrec', 'date']].head())\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(filtered_df['nrec'], bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of Posts with {threshold}+ Recommendations')\n",
    "    plt.xlabel('Number of Recommendations')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nBasic statistics of filtered recommendations:\")\n",
    "    print(filtered_df['nrec'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b867e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Functions\n",
    "def process_recommendations(df, threshold=None):\n",
    "    \"\"\"Process and filter recommendations from the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing posts data\n",
    "        threshold (int): minimum number of recommendations to include\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered DataFrame with nrec >= threshold\n",
    "    \"\"\"\n",
    "    threshold = threshold or CONFIG['DEFAULT_THRESHOLD']\n",
    "    \n",
    "    # Convert recommendations to numeric values\n",
    "    df['nrec'] = pd.to_numeric(\n",
    "        df['nrec'].replace('çˆ†', '100').replace('X', '-1'), \n",
    "        errors='coerce'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Filter posts with recommendations >= threshold\n",
    "    return df[df['nrec'] >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c4226404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping Functions\n",
    "def get_latest_index(board_url=None):\n",
    "    \"\"\"Get the latest page index from PTT board\"\"\"\n",
    "    board_url = board_url or CONFIG['BOARD_URL']\n",
    "    res = safe_request(board_url)\n",
    "    if not res:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    prev_btn = soup.select_one(\"div.btn-group-paging a.btn.wide:nth-child(2)\")\n",
    "    \n",
    "    if prev_btn and 'href' in prev_btn.attrs:\n",
    "        match = re.search(r\"index(\\d+)\\.html\", prev_btn[\"href\"])\n",
    "        if match:\n",
    "            return int(match.group(1)) + 1\n",
    "    return None\n",
    "\n",
    "def get_post_content(url):\n",
    "    \"\"\"Extract content from a single PTT post\"\"\"\n",
    "    res = safe_request(url)\n",
    "    if not res:\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    content_tag = soup.select_one(\"div#main-content\")\n",
    "    \n",
    "    if not content_tag:\n",
    "        return \"\"\n",
    "        \n",
    "    # Remove comments and tags\n",
    "    for tag in content_tag.select(\"div, span\"):\n",
    "        tag.extract()\n",
    "    \n",
    "    return content_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "def crawl_page(index):\n",
    "    \"\"\"Crawl a single page of PTT posts\"\"\"\n",
    "    url = f\"{CONFIG['BASE_URL']}/bbs/Tech_Job/index{index}.html\"\n",
    "    print(f\"\\nðŸ“„ æŠ“å–é é¢: {url}\")\n",
    "    \n",
    "    res = safe_request(url)\n",
    "    if not res:\n",
    "        return []\n",
    "        \n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = []\n",
    "    \n",
    "    for div in soup.select(\"div.r-ent\"):\n",
    "        title_tag = div.select_one(\"div.title > a\")\n",
    "        date_tag = div.select_one(\"div.meta > div.date\")\n",
    "        nrec_tag = div.select_one(\"div.nrec\")\n",
    "        \n",
    "        if title_tag and date_tag:\n",
    "            record = {\n",
    "                \"title\": title_tag.text.strip(),\n",
    "                \"date\": date_tag.text.strip(),\n",
    "                \"link\": f\"{CONFIG['BASE_URL']}{title_tag['href']}\",\n",
    "                \"nrec\": nrec_tag.text.strip() if nrec_tag else \"0\"\n",
    "            }\n",
    "            articles.append(record)\n",
    "            \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d1931056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def safe_request(url, headers=None):\n",
    "    \"\"\"Make a safe HTTP request with error handling\"\"\"\n",
    "    try:\n",
    "        headers = headers or CONFIG['HEADERS']\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.encoding = CONFIG['ENCODING']\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Request failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def random_sleep(range_tuple=None):\n",
    "    \"\"\"Sleep for a random duration within the specified range\"\"\"\n",
    "    sleep_range = range_tuple or CONFIG['SLEEP_TIME_RANGE']\n",
    "    time.sleep(round(random.uniform(*sleep_range), 2))\n",
    "\n",
    "def safe_write_csv(data, filename, mode='a', fieldnames=None):\n",
    "    \"\"\"Safely write data to CSV with proper encoding\"\"\"\n",
    "    try:\n",
    "        with open(filename, mode=mode, encoding='utf-8-sig', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if f.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            if isinstance(data, list):\n",
    "                writer.writerows(data)\n",
    "            else:\n",
    "                writer.writerow(data)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error writing to {filename}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee3df5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Using existing directory for stock at: c:\\Users\\USER\\RAG_learning-project\\stock\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Constants and configurations\n",
    "BOARD_NAME = \"stock\"  # Board name for file organization\n",
    "\n",
    "CONFIG = {\n",
    "    'BOARD_NAME': BOARD_NAME,\n",
    "    'BOARD_URL': f\"https://www.ptt.cc/bbs/{BOARD_NAME}/index.html\",\n",
    "    'BASE_URL': \"https://www.ptt.cc\",\n",
    "    'HEADERS': {\"User-Agent\": \"Mozilla/5.0\"},\n",
    "    'ENCODING': 'utf-8',\n",
    "    'DEFAULT_THRESHOLD': 40,\n",
    "    'SLEEP_TIME_RANGE': (0.1, 0.3),  # Shorter sleep time for general operations\n",
    "    'PAGE_SLEEP_TIME_RANGE': (0.2, 0.5),  # Shorter sleep time between pages\n",
    "    'DATA_DIR': os.path.join(os.getcwd(), BOARD_NAME)  # Create a directory for the board\n",
    "}\n",
    "\n",
    "# Create board directory if it doesn't exist\n",
    "try:\n",
    "    if not os.path.exists(CONFIG['DATA_DIR']):\n",
    "        os.makedirs(CONFIG['DATA_DIR'])\n",
    "        print(f\"âœ… Created new directory for {BOARD_NAME} at: {CONFIG['DATA_DIR']}\")\n",
    "    else:\n",
    "        print(f\"ðŸ“‚ Using existing directory for {BOARD_NAME} at: {CONFIG['DATA_DIR']}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating directory {CONFIG['DATA_DIR']}: {e}\")\n",
    "    raise  # Re-raise the exception since we need the directory to proceed\n",
    "\n",
    "def get_data_path(filename):\n",
    "    \"\"\"Get the full path for a data file in the board's directory\"\"\"\n",
    "    path = os.path.join(CONFIG['DATA_DIR'], filename)\n",
    "    # Ensure the directory exists (in case it was deleted after initial creation)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "84ced437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def get_latest_index(board_url=None):\n",
    "    \"\"\"Get the latest page index from PTT board\"\"\"\n",
    "    board_url = board_url or \"https://www.ptt.cc/bbs/Tech_Job/index.html\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(board_url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # å–å¾—ä¸Šä¸€é æŒ‰éˆ•é€£çµ\n",
    "    prev_btn = soup.select_one(\"div.btn-group-paging a.btn.wide:nth-child(2)\")\n",
    "    if prev_btn:\n",
    "        href = prev_btn[\"href\"]  # e.g., /bbs/Tech_Job/index6772.html\n",
    "        match = re.search(r\"index(\\d+)\\.html\", href)\n",
    "        if match:\n",
    "            return int(match.group(1)) + 1  # æœ€æ–°é  = ä¸Šä¸€é  + 1\n",
    "    return None\n",
    "\n",
    "def crawl_page(index):\n",
    "    url = f\"https://www.ptt.cc/bbs/Tech_Job/index{index}.html\"\n",
    "    print(f\"\\nðŸ“„ æŠ“å–é é¢: {url}\")\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    for div in soup.select(\"div.r-ent\"):\n",
    "        title_tag = div.select_one(\"div.title > a\")\n",
    "        date_tag = div.select_one(\"div.meta > div.date\")\n",
    "        nrec_tag = div.select_one(\"div.nrec\")\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            record = {\n",
    "                \"title\": title_tag.text.strip(),\n",
    "                \"date\": date_tag.text.strip(),\n",
    "                \"link\": \"https://www.ptt.cc\" + title_tag[\"href\"],\n",
    "                \"nrec\": nrec_tag.text.strip() if nrec_tag else \"0\"\n",
    "            }\n",
    "            articles.append(record)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1b6ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ptt_tech_job.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5e2216b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_recommendations(df, threshold=20):\n",
    "    \"\"\"\n",
    "    Filter posts with recommendations below a threshold\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing posts data\n",
    "        threshold (int): minimum number of recommendations to include\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered DataFrame with nrec < threshold\n",
    "    \"\"\"\n",
    "    # Convert nrec to numeric first\n",
    "    df['nrec'] = pd.to_numeric(df['nrec'].replace('çˆ†', '100').replace('X', '-1'), errors='coerce').fillna(0)\n",
    "    \n",
    "    # Filter and return\n",
    "    return df[df['nrec'] > threshold]\n",
    "\n",
    "def visualize_recommendation_distribution(filtered_df, threshold=20):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of recommendations for filtered data\n",
    "    \n",
    "    Args:\n",
    "        filtered_df (pandas.DataFrame): DataFrame containing filtered posts\n",
    "        threshold (int): Maximum value for visualization bins\n",
    "    \"\"\"\n",
    "    # Display statistics\n",
    "    print(f\"Posts with less than {threshold} recommendations:\")\n",
    "    print(f\"Total count: {len(filtered_df)}\")\n",
    "    print(\"\\nFiltered data preview:\")\n",
    "    print(filtered_df[['title', 'nrec', 'date']].head())\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(filtered_df['nrec'], bins=threshold, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of Posts with < {threshold} Recommendations')\n",
    "    plt.xlabel('Number of Recommendations')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nBasic statistics of filtered recommendations:\")\n",
    "    print(filtered_df['nrec'].describe())\n",
    "\n",
    "# Example usage:\n",
    "threshold = 20\n",
    "filtered_data = filter_low_recommendations(df, threshold)\n",
    "#visualize_recommendation_distribution(filtered_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8926c735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully saved 122 records to ptt_tech_job_below_20_rec.csv\n",
      "ðŸ“ File size: 15.1 KB\n"
     ]
    }
   ],
   "source": [
    "def save_filtered_data(filtered_df, threshold=20, filename=None):\n",
    "    \"\"\"\n",
    "    Save filtered recommendations data to CSV\n",
    "    \n",
    "    Args:\n",
    "        filtered_df (pandas.DataFrame): DataFrame containing filtered posts\n",
    "        threshold (int): Threshold used for filtering (default: 20)\n",
    "        filename (str): Optional custom filename (default: None)\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = f'ptt_tech_job_below_{threshold}_rec.csv'\n",
    "    \n",
    "    try:\n",
    "        # Save to CSV with UTF-8-SIG encoding for Chinese characters\n",
    "        filtered_df.to_csv(filename, encoding='utf-8-sig', index=False)\n",
    "        print(f\"âœ… Successfully saved {len(filtered_df)} records to {filename}\")\n",
    "        \n",
    "        # Show file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # Convert to KB\n",
    "        print(f\"ðŸ“ File size: {file_size:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "\n",
    "save_filtered_data(filtered_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "902c2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_structured_content(url):\n",
    "    \"\"\"Extract structured content from a PTT post\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"title\": str,\n",
    "            \"source\": str,\n",
    "            \"urls\": list[str],\n",
    "            \"content\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    res = safe_request(url)\n",
    "    if not res:\n",
    "        return {}\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    main_content = soup.select_one(\"div#main-content\")\n",
    "    if not main_content:\n",
    "        return {}\n",
    "\n",
    "    # ç§»é™¤æŽ¨æ–‡å€\n",
    "    for tag in main_content.find_all(['div', 'span'], recursive=False):\n",
    "        tag.extract()\n",
    "\n",
    "    text = main_content.get_text(separator=\"\\n\").strip()\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    \n",
    "    # æ“·å–æ‰€æœ‰ç¶²å€\n",
    "    urls = [a['href'] for a in main_content.find_all('a', href=True)]\n",
    "\n",
    "    # è§£æžå…§æ–‡çµæ§‹\n",
    "    title = \"\"\n",
    "    source = \"\"\n",
    "    content_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        if \"æ¨™é¡Œ\" in line or \"æ¨™é¡Œ:\" in line:\n",
    "            title = re.sub(r'^.*?æ¨™é¡Œ[:,ï¼š]\\s*', '', line).strip()\n",
    "        elif \"ä½œè€…\" in line or \"ä½œè€…:\" in line:\n",
    "            source = re.sub(r'^.*?ä½œè€…[:,ï¼š]\\s*', '', line).strip()\n",
    "        else:\n",
    "            content_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    # æ¸…ç†å…§æ–‡ï¼Œç§»é™¤é¡å¤–çš„ header è³‡è¨Š\n",
    "    clean_content = []\n",
    "    content_started = False\n",
    "    for line in content_lines:\n",
    "        if not content_started:\n",
    "            if \"çœ‹æ¿\" in line or \"æ™‚é–“\" in line:\n",
    "                continue\n",
    "            content_started = True\n",
    "        clean_content.append(line)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"source\": source,\n",
    "        \"urls\": urls,\n",
    "        \"content\": \"\\n\".join(clean_content).strip()\n",
    "    }\n",
    "\n",
    "def crawl_ptt_post_content(filtered_data: pd.DataFrame, output_file: str = None):\n",
    "    \"\"\"\n",
    "    å¾ž PTT å·²ç¯©é¸çš„æ–‡ç« ä¸­æŠ“å–æ¯ç¯‡çµæ§‹åŒ–å…§æ–‡ï¼Œä¸¦å„²å­˜è‡³ CSVã€‚\n",
    "\n",
    "    Args:\n",
    "        filtered_data (pd.DataFrame): å«æœ‰ title/date/link/nrec çš„è³‡æ–™\n",
    "        output_file (str): è¼¸å‡ºæª”æ¡ˆåç¨±\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = get_data_path(f'{CONFIG[\"BOARD_NAME\"]}_content.csv')\n",
    "    \n",
    "    # Check existing data\n",
    "    existing_links = set()\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            existing_links = set(existing_df[\"link\"].dropna().tolist())\n",
    "            print(f\"ðŸ“ å·²å­˜åœ¨è³‡æ–™ç­†æ•¸ï¼š{len(existing_links)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ç„¡æ³•è®€å– {output_file}ï¼ŒéŒ¯èª¤ï¼š{e}\")\n",
    "\n",
    "    # Filter new links\n",
    "    to_crawl = filtered_data[~filtered_data[\"link\"].isin(existing_links)]\n",
    "    print(f\"ðŸš€ æº–å‚™æŠ“å–æ–°é€£çµç­†æ•¸ï¼š{len(to_crawl)}\")\n",
    "\n",
    "    # Process each post\n",
    "    fieldnames = [\"title\", \"date\", \"link\", \"nrec\", \"source\", \"content\", \"urls\"]\n",
    "    for _, row in tqdm(to_crawl.iterrows(), total=len(to_crawl)):\n",
    "        structured_content = get_structured_content(row[\"link\"])\n",
    "        \n",
    "        record = {\n",
    "            \"title\": row[\"title\"],\n",
    "            \"date\": row[\"date\"],\n",
    "            \"link\": row[\"link\"],\n",
    "            \"nrec\": row[\"nrec\"],\n",
    "            \"source\": structured_content.get(\"source\", \"\"),\n",
    "            \"content\": structured_content.get(\"content\", \"\"),\n",
    "            \"urls\": \"|\".join(structured_content.get(\"urls\", []))  # Join URLs with pipe separator\n",
    "        }\n",
    "        \n",
    "        safe_write_csv(record, output_file, fieldnames=fieldnames)\n",
    "        random_sleep()\n",
    "\n",
    "    print(\"âœ… å…¨éƒ¨å…§æ–‡æŠ“å–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead4b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Data will be saved in: c:\\Users\\USER\\RAG_learning-project\\stock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47fc5e821f840139ec3bd788e332e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index4002.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index4001.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index4001.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index4000.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index4000.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index3999.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index3999.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index3998.html\n",
      "\n",
      "ðŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index3998.html\n",
      "âœ… Raw data saved to stock_raw.csv\n",
      "âœ… Successfully saved 4 records with 40+ recommendations to c:\\Users\\USER\\RAG_learning-project\\stock\\stock_above_40_rec.csv\n",
      "ðŸ“ File size: 0.5 KB\n",
      "ðŸ“ å·²å­˜åœ¨è³‡æ–™ç­†æ•¸ï¼š13\n",
      "ðŸš€ æº–å‚™æŠ“å–æ–°é€£çµç­†æ•¸ï¼š0\n",
      "âœ… Raw data saved to stock_raw.csv\n",
      "âœ… Successfully saved 4 records with 40+ recommendations to c:\\Users\\USER\\RAG_learning-project\\stock\\stock_above_40_rec.csv\n",
      "ðŸ“ File size: 0.5 KB\n",
      "ðŸ“ å·²å­˜åœ¨è³‡æ–™ç­†æ•¸ï¼š13\n",
      "ðŸš€ æº–å‚™æŠ“å–æ–°é€£çµç­†æ•¸ï¼š0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a29417f2f784176acb33ca406028441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å…¨éƒ¨å…§æ–‡æŠ“å–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"ðŸ“‚ Data will be saved in: {CONFIG['DATA_DIR']}\")\n",
    "    \n",
    "    # 1. Crawl posts\n",
    "    df = crawl_posts(num_pages=10)\n",
    "    \n",
    "    # Save raw data\n",
    "    raw_filename = f'{CONFIG[\"BOARD_NAME\"]}_raw.csv'\n",
    "    df.to_csv(get_data_path(raw_filename), encoding='utf-8-sig', index=False)\n",
    "    print(f\"âœ… Raw data saved to {raw_filename}\")\n",
    "    \n",
    "    # 2. Process and save filtered data\n",
    "    filtered_df = process_and_save_data(df)\n",
    "    \n",
    "    # 3. Visualize results\n",
    "    #visualize_recommendations(filtered_df)\n",
    "    \n",
    "    # 4. Crawl post contents\n",
    "    crawl_posts_content(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c864397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample of cleaned content (4th row) ===\n",
      "ETtodayæ–°èžé›² 2025å¹´07æœˆ18æ—¥ 16:23\n",
      "å¿«è¨Šï¼å°ç©é›»å˜‰ç¾©å» ã€Œ2æœˆ4èµ·å·¥å®‰æ„å¤–ã€ã€€å¥³å­é­ç™¾å…¬æ–¤å†°æ°´ç®¡ç ¸æ­»\n",
      "è¨˜è€…ç¿ä¼Šæ£®ã€é»ƒè³‡çœŸï¼å˜‰ç¾©å ±å°Ž\n",
      "ï¼Œ\n",
      "ç´°æ•¸é€™å…©å€‹æœˆä¾†çš„å·¥å®‰æ„å¤–ï¼Œ\n",
      "ï¼›\n",
      "ï¼Œ\n",
      "ï¼›\n",
      "ï¼Œ2å€‹æœˆå…§çš„4èµ·å·¥å®‰äº‹æ•…ï¼Œå·²é€ æˆ2æ­»2å‚·ã€‚\n",
      "https://www.ettoday.net/news/20250718/2998843.htm\n",
      "æ€•\n",
      "2å€‹æœˆ4èµ·å·¥å®‰äº‹æ•…  é›–ç„¶é€™æ˜¯å¤–åŒ…å» å•†ä½†å·¥å®‰äº‹æ•…ä¹Ÿç™¼ç”Ÿå¤ªé »ç¹äº†\n",
      "ä¸Šæ¬¡å¥½åƒåœå·¥ä¸€å€‹æœˆ  é€™æ¬¡æ„Ÿè¦ºæ‡‰è©²ä¹Ÿæ˜¯åœä¸€å€‹æœˆ?\n",
      "é€™æ¨£ä¸‹åŽ»å˜‰ç¾©å» æœ‰è¾¦æ³•æ™‚é–“å…§å®Œå·¥?\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Clean and filter content based on recommendations\n",
    "def clean_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace multiple newlines with a single newline\n",
    "    cleaned = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    # Remove leading/trailing whitespace and extra newlines\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "# Read the content data\n",
    "result = pd.read_csv(get_data_path(f'{CONFIG[\"BOARD_NAME\"]}_content.csv'))\n",
    "\n",
    "# Clean the content column\n",
    "result['content'] = result['content'].apply(clean_content)\n",
    "\n",
    "# Convert recommendations to numeric values\n",
    "result['nrec'] = pd.to_numeric(\n",
    "    result['nrec'].replace('çˆ†', '100').replace('X', '-1'), \n",
    "    errors='coerce'\n",
    ").fillna(0)\n",
    "\n",
    "# Filter and save data for different thresholds\n",
    "thresholds = [20, 40]  # You can modify these thresholds as needed\n",
    "for threshold in thresholds:\n",
    "    # Filter data below threshold\n",
    "    filtered_df = result[result['nrec'] < threshold]\n",
    "    # Save filtered data\n",
    "    output_filename = f'{CONFIG[\"BOARD_NAME\"]}_below_{threshold}_rec.csv'\n",
    "    filtered_df.to_csv(get_data_path(output_filename), encoding='utf-8-sig', index=False)\n",
    "    print(f\"âœ… Saved {len(filtered_df)} records below {threshold} recommendations to {output_filename}\")\n",
    "\n",
    "# Display a sample of cleaned content\n",
    "print(\"\\n=== Sample of cleaned content (4th row) ===\")\n",
    "print(result.iloc[3]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd3b9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned full data back to the CSV\n",
    "result.to_csv(get_data_path(f'{CONFIG[\"BOARD_NAME\"]}_content.csv'), encoding='utf-8-sig', index=False)\n",
    "print(f\"âœ… Saved {len(result)} records to {CONFIG['BOARD_NAME']}_content.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
