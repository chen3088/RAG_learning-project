{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ced437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 抓取頁面: https://www.ptt.cc/bbs/Tech_Job/index4001.html\n",
      "✅ 寫入: [新聞] 黃仁勳大讚：中國AI崛起 沒有輝達也行\n",
      "✅ 寫入: [公告] op1984 livehouse emarl5566 水桶一個月\n",
      "✅ 寫入: [公告] eokuan 水桶一年\n",
      "✅ 寫入: [討論] 有人知道鈺太科技這家公司嗎\n",
      "✅ 寫入: [新聞] 經長︰爭取談到有競爭力的關稅\n",
      "✅ 寫入: [討論] 高中生科展做1.5nm GAA 可以去台積RD嗎?\n",
      "✅ 寫入: [公告] hinesmile romeie06 水桶一個月\n",
      "✅ 寫入: [公告] teamax cityhunter04 水桶\n",
      "✅ 寫入: [情報] 免費晶片安全實戰課程- 2 天\n",
      "✅ 寫入: Re: [討論] 高中生科展做1.5nm GAA 可以去台積RD嗎?\n",
      "✅ 寫入: [新聞] AI取代人力？人力銀行：企業估31%工作恐\n",
      "✅ 寫入: [新聞] AI新十大建設 打造新護國群山\n",
      "✅ 寫入: [新聞]三星困境都怪自己！2018 年黃仁勳曾上門尋\n",
      "✅ 寫入: [公告] godman362 xabcxabc0123 Ferscism 水桶\n",
      "✅ 寫入: Re: [討論] 高中生科展做1.5nm GAA 可以去台積RD嗎?\n",
      "✅ 寫入: [新聞] AMD採購台積電美製晶片成本高　蘇姿丰：\n",
      "✅ 寫入: Re: [討論] 高中生科展做1.5nm GAA 可以去台積RD\n",
      "✅ 寫入: [公告] zrc888 水桶一年\n",
      "✅ 寫入: [情報] 薪資查詢平台\n",
      "✅ 寫入: [公告] 多重帳號違規處理說明與適用原則\n",
      "✅ 寫入: [公告] 科技板（Tech_Job）板規2025版\n",
      "✅ 寫入: [公告] 跑步哥列本板不受歡迎人物\n",
      "✅ 寫入: [公告] 本版徵新版主\n",
      "\n",
      "📄 抓取頁面: https://www.ptt.cc/bbs/Tech_Job/index4000.html\n",
      "✅ 寫入: [請益] 有人覺得台積電薪水給太低嗎\n",
      "✅ 寫入: [請益] 元鈦科技（散熱）文化好嗎？\n",
      "✅ 寫入: Re: [請益] 有人覺得台積電薪水給太低嗎\n",
      "✅ 寫入: [新聞] 輝達解禁 盧特尼克：讓中對美技術上癮\n",
      "✅ 寫入: Re: [新聞] 徵才鎖定AI技能 6成企業願加薪最高3千\n",
      "✅ 寫入: [新聞] 快訊／台積電嘉義廠「2月4起工安意外」\n",
      "✅ 寫入: Re: [新聞] 工程師年薪300萬「卻騎車上班？」妹子不解　竹科人曝悲\n",
      "✅ 寫入: [新聞] 黃仁勳：華為AI晶片取代輝達是時間問題\n",
      "✅ 寫入: [新聞] 擴大技術領先 台積2奈米良率超預期\n",
      "✅ 寫入: [請益] 台積電的股價是一堆工程師輪班換來的嗎\n",
      "✅ 寫入: [請益] Offer請益\n",
      "✅ 寫入: [新聞] AI先驅黃仁勳 獲讚當今最受尊敬科技巨頭\n",
      "✅ 寫入: [新聞] 台積電嘉科廠工安意外 停工\n",
      "✅ 寫入: [公告] whizz G8AJ 水桶一個月\n",
      "✅ 寫入: Re: [新聞] 工程師年薪300萬「卻騎車上班？」妹子不解　竹科人曝悲\n",
      "✅ 寫入: [新聞] 李文如閃辭傳接任輝達台灣區總經理！台積\n",
      "✅ 寫入: Re: [新聞] 工程師年薪300萬「卻騎車上班？」妹子不解　竹科人曝悲\n",
      "✅ 寫入: [新聞] 經部：台積電最先進技術根留台灣　勿散\n",
      "\n",
      "📄 抓取頁面: https://www.ptt.cc/bbs/Tech_Job/index3999.html\n",
      "✅ 寫入: Re: [新聞] 敏感詞達35萬 DeepSeek使用率從50%跌至3%\n",
      "✅ 寫入: [公告] 多帳號鬧版臨時版規\n",
      "✅ 寫入: [公告] xenorock 版規四 水桶一個月\n",
      "✅ 寫入: [新聞] 台歐盟勞動諮商會議 談尊嚴勞動、社會對\n",
      "✅ 寫入: [新聞] 徵才鎖定AI技能 6成企業願加薪最高3千\n",
      "✅ 寫入: [新聞]普鴻 25 週年三大新品搶市！董座林群國：\n",
      "✅ 寫入: [公告] allmight7912 水桶一個月\n",
      "✅ 寫入: [請益] 台積AP廠應徵\n",
      "✅ 寫入: ［請益］詢問美光的面試流程\n",
      "✅ 寫入: [新聞] 不讓輝達獨霸市場！博通推新款「戰斧」AI\n",
      "✅ 寫入: [新聞] 工程師年薪300萬「卻騎車上班？」妹子不解　竹科人曝悲\n",
      "✅ 寫入: [新聞] 何麗梅卸任台積電人資長 由陳培宏接任\n",
      "✅ 寫入: [新聞] ASML 示警 2026 年不確定維持成長，利空\n",
      "✅ 寫入: [新聞] 黃仁勳：輕視華為、中國製造的人都極其\n",
      "✅ 寫入: [新聞] 快訊／台積電嘉義廠又爆工安事故　工人遭「百公斤冰水管」砸\n",
      "✅ 寫入: [公告] R3hab iverson909 lwamp lovefisk 政治文 水桶一個月\n",
      "✅ 寫入: Re: [新聞] 黃仁勳：輕視華為、中國製造的人都極其\n",
      "✅ 寫入: [新聞]台積電美國學徒計畫，漢堡店員工變身半導\n",
      "✅ 寫入: [公告] longman12579 jengmei 水桶\n",
      "✅ 寫入: [新聞] 快訊／史上最強！台積電Q2賺3983億年增61\n",
      "\n",
      "📄 抓取頁面: https://www.ptt.cc/bbs/Tech_Job/index3998.html\n",
      "✅ 寫入: [新聞] 微軟培訓AI人才 擬砸40億美元\n",
      "✅ 寫入: [新聞] 英特爾前執行長再出發，閃電現身東京，Pa\n",
      "✅ 寫入: [公告] ilovetaida 水桶一個月\n",
      "✅ 寫入: [公告] encyclopedia 水桶一個月\n",
      "✅ 寫入: ［請益］offer 請益\n",
      "✅ 寫入: Re: [公告] 跑步哥列本板不受歡迎人物\n",
      "✅ 寫入: [公告] iamala 政治文 水桶一個月\n",
      "✅ 寫入: [請益] 力成科技 客戶品質工程師\n",
      "✅ 寫入: Re: [公告] 跑步哥列本板不受歡迎人物\n",
      "✅ 寫入: Re: [公告] 跑步哥列本板不受歡迎人物\n",
      "✅ 寫入: [公告] D600dust 水桶三十日\n",
      "✅ 寫入: [新聞] 黃仁勳：美科技公司 挺得過川普關稅\n",
      "✅ 寫入: [新聞] 快訊／高雄三元能源科技大火！　14男1女\n",
      "✅ 寫入: [公告] 本版徵新版主\n",
      "✅ 寫入: [公告] Dino14124 AreLies 水桶一個月\n",
      "✅ 寫入: [討論] 大家會有指導獎金嗎\n",
      "✅ 寫入: [新聞] 緯創內湖研發大樓動工 打造AI科技廊道新\n",
      "✅ 寫入: Re: [新聞] 敏感詞達35萬 DeepSeek使用率從50%跌至3%\n",
      "✅ 寫入: [新聞]日本政府傳要求持有晶圓代工廠 Rapidus「\n",
      "✅ 寫入: [新聞] 台積電中生代主管搬風\n",
      "\n",
      "📄 抓取頁面: https://www.ptt.cc/bbs/Tech_Job/index3997.html\n",
      "✅ 寫入: [公告] boards 版規4 水桶一個月\n",
      "✅ 寫入: [公告] a790813a 商業廣告 水桶一個月\n",
      "✅ 寫入: Fw: [公告] 7/12 停機公告\n",
      "✅ 寫入: [請益] 5年業務經驗 科技業務請益履歷與職缺\n",
      "✅ 寫入: [討論] 台灣微軟這次有人被裁員嗎？\n",
      "✅ 寫入: [公告]多重分身水桶\n",
      "✅ 寫入: [公告] 台達公關檢舉區\n",
      "✅ 寫入: [新聞] 再關閉南科五廠最晚明年中停產？群創：稍\n",
      "✅ 寫入: [討論] GG好像要統治南部了\n",
      "✅ 寫入: [新聞] 全球首家！輝達市值突破4兆美元大關\n",
      "✅ 寫入: Re: [公告] 台達公關檢舉區\n",
      "✅ 寫入: Fw: 提醒一些事情。\n",
      "✅ 寫入: [新聞]「信任」兩字太沉重！中華電董座反駁「世\n",
      "✅ 寫入: Fw: 請注意自己的身心狀況\n",
      "✅ 寫入: Re: [公告]多重分身水桶\n",
      "✅ 寫入: [公告] 跑步哥列本板不受歡迎人物\n",
      "✅ 寫入: [公告] ctfjuw 水桶一個月\n",
      "✅ 寫入: Re: [新聞] 台積電掌管採購副總李文如休長假\n",
      "✅ 寫入: [新聞] Google薪資流出！軟體工程師破1000萬、UX\n",
      "✅ 寫入: [新聞] 敏感詞達35萬 DeepSeek使用率從50%跌至3%\n",
      "\n",
      "🎉 共處理 5 頁完成！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def get_latest_index(board_url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(board_url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # 取得上一頁按鈕連結\n",
    "    prev_btn = soup.select_one(\"div.btn-group-paging a.btn.wide:nth-child(2)\")\n",
    "    if prev_btn:\n",
    "        href = prev_btn[\"href\"]  # e.g., /bbs/Tech_Job/index6772.html\n",
    "        match = re.search(r\"index(\\d+)\\.html\", href)\n",
    "        if match:\n",
    "            return int(match.group(1)) + 1  # 最新頁 = 上一頁 + 1\n",
    "    return None\n",
    "\n",
    "def crawl_page(index):\n",
    "    url = f\"https://www.ptt.cc/bbs/Tech_Job/index{index}.html\"\n",
    "    print(f\"\\n📄 抓取頁面: {url}\")\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    for div in soup.select(\"div.r-ent\"):\n",
    "        title_tag = div.select_one(\"div.title > a\")\n",
    "        date_tag = div.select_one(\"div.meta > div.date\")\n",
    "        nrec_tag = div.select_one(\"div.nrec\")\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            record = {\n",
    "                \"title\": title_tag.text.strip(),\n",
    "                \"date\": date_tag.text.strip(),\n",
    "                \"link\": \"https://www.ptt.cc\" + title_tag[\"href\"],\n",
    "                \"nrec\": nrec_tag.text.strip() if nrec_tag else \"0\"\n",
    "            }\n",
    "            articles.append(record)\n",
    "    return articles\n",
    "\n",
    "def main():\n",
    "    num_pages = 5  # 要往前抓幾頁\n",
    "    board_url = \"https://www.ptt.cc/bbs/Tech_Job/index.html\"\n",
    "    latest_index = get_latest_index(board_url)\n",
    "\n",
    "    if not latest_index:\n",
    "        print(\"❌ 無法取得最新頁碼\")\n",
    "        return\n",
    "\n",
    "    # 開啟 CSV（追加寫入）\n",
    "    with open(\"ptt_tech_job.csv\", mode='a', encoding='utf-8-sig', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\"title\", \"date\", \"link\", \"nrec\"])\n",
    "        if csv_file.tell() == 0:  # 若為新檔案，寫欄位名\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 從最新頁往前 N 頁\n",
    "        for page_index in range(latest_index, latest_index - num_pages, -1):\n",
    "            try:\n",
    "                articles = crawl_page(page_index)\n",
    "                for record in articles:\n",
    "                    writer.writerow(record)\n",
    "                    print(f\"✅ 寫入: {record['title']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 發生錯誤於 index{page_index}: {e}\")\n",
    "            \n",
    "            # 翻頁後等待\n",
    "            time.sleep(random.uniform(1.0, 2.5))\n",
    "\n",
    "    print(f\"\\n🎉 共處理 {num_pages} 頁完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b6ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ptt_tech_job.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2216b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_recommendations(df, threshold=20):\n",
    "    \"\"\"\n",
    "    Filter posts with recommendations below a threshold\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing posts data\n",
    "        threshold (int): minimum number of recommendations to include\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered DataFrame with nrec < threshold\n",
    "    \"\"\"\n",
    "    # Convert nrec to numeric first\n",
    "    df['nrec'] = pd.to_numeric(df['nrec'].replace('爆', '100').replace('X', '-1'), errors='coerce').fillna(0)\n",
    "    \n",
    "    # Filter and return\n",
    "    return df[df['nrec'] > threshold]\n",
    "\n",
    "def visualize_recommendation_distribution(filtered_df, threshold=20):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of recommendations for filtered data\n",
    "    \n",
    "    Args:\n",
    "        filtered_df (pandas.DataFrame): DataFrame containing filtered posts\n",
    "        threshold (int): Maximum value for visualization bins\n",
    "    \"\"\"\n",
    "    # Display statistics\n",
    "    print(f\"Posts with less than {threshold} recommendations:\")\n",
    "    print(f\"Total count: {len(filtered_df)}\")\n",
    "    print(\"\\nFiltered data preview:\")\n",
    "    print(filtered_df[['title', 'nrec', 'date']].head())\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(filtered_df['nrec'], bins=threshold, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of Posts with < {threshold} Recommendations')\n",
    "    plt.xlabel('Number of Recommendations')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nBasic statistics of filtered recommendations:\")\n",
    "    print(filtered_df['nrec'].describe())\n",
    "\n",
    "# Example usage:\n",
    "threshold = 20\n",
    "filtered_data = filter_low_recommendations(df, threshold)\n",
    "#visualize_recommendation_distribution(filtered_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8926c735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved 65 records to ptt_tech_job_below_20_rec.csv\n",
      "📁 File size: 7.8 KB\n"
     ]
    }
   ],
   "source": [
    "def save_filtered_data(filtered_df, threshold=20, filename=None):\n",
    "    \"\"\"\n",
    "    Save filtered recommendations data to CSV\n",
    "    \n",
    "    Args:\n",
    "        filtered_df (pandas.DataFrame): DataFrame containing filtered posts\n",
    "        threshold (int): Threshold used for filtering (default: 20)\n",
    "        filename (str): Optional custom filename (default: None)\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = f'ptt_tech_job_below_{threshold}_rec.csv'\n",
    "    \n",
    "    try:\n",
    "        # Save to CSV with UTF-8-SIG encoding for Chinese characters\n",
    "        filtered_df.to_csv(filename, encoding='utf-8-sig', index=False)\n",
    "        print(f\"✅ Successfully saved {len(filtered_df)} records to {filename}\")\n",
    "        \n",
    "        # Show file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # Convert to KB\n",
    "        print(f\"📁 File size: {file_size:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving file: {e}\")\n",
    "\n",
    "save_filtered_data(filtered_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902c2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "def get_post_content(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        content_tag = soup.select_one(\"div#main-content\")\n",
    "        if not content_tag:\n",
    "            return \"\"\n",
    "\n",
    "        # 移除推文與標籤\n",
    "        for tag in content_tag.select(\"div, span\"):\n",
    "            tag.extract()\n",
    "\n",
    "        text = content_tag.get_text(separator=\"\\n\").strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 抓取失敗: {url} | 錯誤: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def crawl_ptt_post_content(filtered_data: pd.DataFrame, output_file: str = \"ptt_tech_job_content.csv\"):\n",
    "    \"\"\"\n",
    "    從 PTT 已篩選的文章中抓取每篇內文，並儲存至 CSV，具備避免重複功能。\n",
    "\n",
    "    Args:\n",
    "        filtered_data (pd.DataFrame): 含有 title/date/link/nrec 的資料\n",
    "        output_file (str): 輸出檔案名稱\n",
    "    \"\"\"\n",
    "    # === 讀取已存在的 link 避免重複 ===\n",
    "    existing_links = set()\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            existing_links = set(existing_df[\"link\"].dropna().tolist())\n",
    "            print(f\"📁 已存在資料筆數：{len(existing_links)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 無法讀取 {output_file}，錯誤：{e}\")\n",
    "\n",
    "    # === 篩選尚未處理的連結 ===\n",
    "    to_crawl = filtered_data[~filtered_data[\"link\"].isin(existing_links)]\n",
    "\n",
    "    print(f\"🚀 準備抓取新連結筆數：{len(to_crawl)}\")\n",
    "\n",
    "    # === 寫入 CSV ===\n",
    "    with open(output_file, mode='a', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"date\", \"link\", \"nrec\", \"content\"])\n",
    "        if f.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for _, row in to_crawl.iterrows():\n",
    "            url = row[\"link\"]\n",
    "            print(f\"🔍 抓取內容: {url}\")\n",
    "            content = get_post_content(url)\n",
    "\n",
    "            writer.writerow({\n",
    "                \"title\": row[\"title\"],\n",
    "                \"date\": row[\"date\"],\n",
    "                \"link\": row[\"link\"],\n",
    "                \"nrec\": row[\"nrec\"],\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "            sleep_time = round(random.uniform(0.2, 1.0), 2)\n",
    "            print(f\"⏳ 等待 {sleep_time} 秒...\\n\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    print(\"✅ 全部內文抓取完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 已存在資料筆數：31\n",
      "🚀 準備抓取新連結筆數：0\n",
      "✅ 全部內文抓取完成！\n"
     ]
    }
   ],
   "source": [
    "crawl_ptt_post_content(filtered_data, output_file=\"ptt_tech_job_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead4b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
