{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ced437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index4001.html\n",
      "âœ… å¯«å…¥: [æ–°è] é»ƒä»å‹³å¤§è®šï¼šä¸­åœ‹AIå´›èµ· æ²’æœ‰è¼é”ä¹Ÿè¡Œ\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] op1984 livehouse emarl5566 æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] eokuan æ°´æ¡¶ä¸€å¹´\n",
      "âœ… å¯«å…¥: [è¨è«–] æœ‰äººçŸ¥é“éˆºå¤ªç§‘æŠ€é€™å®¶å…¬å¸å—\n",
      "âœ… å¯«å…¥: [æ–°è] ç¶“é•·ï¸°çˆ­å–è«‡åˆ°æœ‰ç«¶çˆ­åŠ›çš„é—œç¨…\n",
      "âœ… å¯«å…¥: [è¨è«–] é«˜ä¸­ç”Ÿç§‘å±•åš1.5nm GAA å¯ä»¥å»å°ç©RDå—?\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] hinesmile romeie06 æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] teamax cityhunter04 æ°´æ¡¶\n",
      "âœ… å¯«å…¥: [æƒ…å ±] å…è²»æ™¶ç‰‡å®‰å…¨å¯¦æˆ°èª²ç¨‹- 2 å¤©\n",
      "âœ… å¯«å…¥: Re: [è¨è«–] é«˜ä¸­ç”Ÿç§‘å±•åš1.5nm GAA å¯ä»¥å»å°ç©RDå—?\n",
      "âœ… å¯«å…¥: [æ–°è] AIå–ä»£äººåŠ›ï¼ŸäººåŠ›éŠ€è¡Œï¼šä¼æ¥­ä¼°31%å·¥ä½œæ\n",
      "âœ… å¯«å…¥: [æ–°è] AIæ–°åå¤§å»ºè¨­ æ‰“é€ æ–°è­·åœ‹ç¾¤å±±\n",
      "âœ… å¯«å…¥: [æ–°è]ä¸‰æ˜Ÿå›°å¢ƒéƒ½æ€ªè‡ªå·±ï¼2018 å¹´é»ƒä»å‹³æ›¾ä¸Šé–€å°‹\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] godman362 xabcxabc0123 Ferscism æ°´æ¡¶\n",
      "âœ… å¯«å…¥: Re: [è¨è«–] é«˜ä¸­ç”Ÿç§‘å±•åš1.5nm GAA å¯ä»¥å»å°ç©RDå—?\n",
      "âœ… å¯«å…¥: [æ–°è] AMDæ¡è³¼å°ç©é›»ç¾è£½æ™¶ç‰‡æˆæœ¬é«˜ã€€è˜‡å§¿ä¸°ï¼š\n",
      "âœ… å¯«å…¥: Re: [è¨è«–] é«˜ä¸­ç”Ÿç§‘å±•åš1.5nm GAA å¯ä»¥å»å°ç©RD\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] zrc888 æ°´æ¡¶ä¸€å¹´\n",
      "âœ… å¯«å…¥: [æƒ…å ±] è–ªè³‡æŸ¥è©¢å¹³å°\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] å¤šé‡å¸³è™Ÿé•è¦è™•ç†èªªæ˜èˆ‡é©ç”¨åŸå‰‡\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] ç§‘æŠ€æ¿ï¼ˆTech_Jobï¼‰æ¿è¦2025ç‰ˆ\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] è·‘æ­¥å“¥åˆ—æœ¬æ¿ä¸å—æ­¡è¿äººç‰©\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] æœ¬ç‰ˆå¾µæ–°ç‰ˆä¸»\n",
      "\n",
      "ğŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index4000.html\n",
      "âœ… å¯«å…¥: [è«‹ç›Š] æœ‰äººè¦ºå¾—å°ç©é›»è–ªæ°´çµ¦å¤ªä½å—\n",
      "âœ… å¯«å…¥: [è«‹ç›Š] å…ƒéˆ¦ç§‘æŠ€ï¼ˆæ•£ç†±ï¼‰æ–‡åŒ–å¥½å—ï¼Ÿ\n",
      "âœ… å¯«å…¥: Re: [è«‹ç›Š] æœ‰äººè¦ºå¾—å°ç©é›»è–ªæ°´çµ¦å¤ªä½å—\n",
      "âœ… å¯«å…¥: [æ–°è] è¼é”è§£ç¦ ç›§ç‰¹å°¼å…‹ï¼šè®“ä¸­å°ç¾æŠ€è¡“ä¸Šç™®\n",
      "âœ… å¯«å…¥: Re: [æ–°è] å¾µæ‰é–å®šAIæŠ€èƒ½ 6æˆä¼æ¥­é¡˜åŠ è–ªæœ€é«˜3åƒ\n",
      "âœ… å¯«å…¥: [æ–°è] å¿«è¨Šï¼å°ç©é›»å˜‰ç¾©å» ã€Œ2æœˆ4èµ·å·¥å®‰æ„å¤–ã€\n",
      "âœ… å¯«å…¥: Re: [æ–°è] å·¥ç¨‹å¸«å¹´è–ª300è¬ã€Œå»é¨è»Šä¸Šç­ï¼Ÿã€å¦¹å­ä¸è§£ã€€ç«¹ç§‘äººæ›æ‚²\n",
      "âœ… å¯«å…¥: [æ–°è] é»ƒä»å‹³ï¼šè¯ç‚ºAIæ™¶ç‰‡å–ä»£è¼é”æ˜¯æ™‚é–“å•é¡Œ\n",
      "âœ… å¯«å…¥: [æ–°è] æ“´å¤§æŠ€è¡“é ˜å…ˆ å°ç©2å¥ˆç±³è‰¯ç‡è¶…é æœŸ\n",
      "âœ… å¯«å…¥: [è«‹ç›Š] å°ç©é›»çš„è‚¡åƒ¹æ˜¯ä¸€å †å·¥ç¨‹å¸«è¼ªç­æ›ä¾†çš„å—\n",
      "âœ… å¯«å…¥: [è«‹ç›Š] Offerè«‹ç›Š\n",
      "âœ… å¯«å…¥: [æ–°è] AIå…ˆé©…é»ƒä»å‹³ ç²è®šç•¶ä»Šæœ€å—å°Šæ•¬ç§‘æŠ€å·¨é ­\n",
      "âœ… å¯«å…¥: [æ–°è] å°ç©é›»å˜‰ç§‘å» å·¥å®‰æ„å¤– åœå·¥\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] whizz G8AJ æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: Re: [æ–°è] å·¥ç¨‹å¸«å¹´è–ª300è¬ã€Œå»é¨è»Šä¸Šç­ï¼Ÿã€å¦¹å­ä¸è§£ã€€ç«¹ç§‘äººæ›æ‚²\n",
      "âœ… å¯«å…¥: [æ–°è] ææ–‡å¦‚é–ƒè¾­å‚³æ¥ä»»è¼é”å°ç£å€ç¸½ç¶“ç†ï¼å°ç©\n",
      "âœ… å¯«å…¥: Re: [æ–°è] å·¥ç¨‹å¸«å¹´è–ª300è¬ã€Œå»é¨è»Šä¸Šç­ï¼Ÿã€å¦¹å­ä¸è§£ã€€ç«¹ç§‘äººæ›æ‚²\n",
      "âœ… å¯«å…¥: [æ–°è] ç¶“éƒ¨ï¼šå°ç©é›»æœ€å…ˆé€²æŠ€è¡“æ ¹ç•™å°ç£ã€€å‹¿æ•£\n",
      "\n",
      "ğŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index3999.html\n",
      "âœ… å¯«å…¥: Re: [æ–°è] æ•æ„Ÿè©é”35è¬ DeepSeekä½¿ç”¨ç‡å¾50%è·Œè‡³3%\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] å¤šå¸³è™Ÿé¬§ç‰ˆè‡¨æ™‚ç‰ˆè¦\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] xenorock ç‰ˆè¦å›› æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [æ–°è] å°æ­ç›Ÿå‹å‹•è«®å•†æœƒè­° è«‡å°Šåš´å‹å‹•ã€ç¤¾æœƒå°\n",
      "âœ… å¯«å…¥: [æ–°è] å¾µæ‰é–å®šAIæŠ€èƒ½ 6æˆä¼æ¥­é¡˜åŠ è–ªæœ€é«˜3åƒ\n",
      "âœ… å¯«å…¥: [æ–°è]æ™®é´» 25 é€±å¹´ä¸‰å¤§æ–°å“æ¶å¸‚ï¼è‘£åº§æ—ç¾¤åœ‹ï¼š\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] allmight7912 æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [è«‹ç›Š] å°ç©APå» æ‡‰å¾µ\n",
      "âœ… å¯«å…¥: ï¼»è«‹ç›Šï¼½è©¢å•ç¾å…‰çš„é¢è©¦æµç¨‹\n",
      "âœ… å¯«å…¥: [æ–°è] ä¸è®“è¼é”ç¨éœ¸å¸‚å ´ï¼åšé€šæ¨æ–°æ¬¾ã€Œæˆ°æ–§ã€AI\n",
      "âœ… å¯«å…¥: [æ–°è] å·¥ç¨‹å¸«å¹´è–ª300è¬ã€Œå»é¨è»Šä¸Šç­ï¼Ÿã€å¦¹å­ä¸è§£ã€€ç«¹ç§‘äººæ›æ‚²\n",
      "âœ… å¯«å…¥: [æ–°è] ä½•éº—æ¢…å¸ä»»å°ç©é›»äººè³‡é•· ç”±é™³åŸ¹å®æ¥ä»»\n",
      "âœ… å¯«å…¥: [æ–°è] ASML ç¤ºè­¦ 2026 å¹´ä¸ç¢ºå®šç¶­æŒæˆé•·ï¼Œåˆ©ç©º\n",
      "âœ… å¯«å…¥: [æ–°è] é»ƒä»å‹³ï¼šè¼•è¦–è¯ç‚ºã€ä¸­åœ‹è£½é€ çš„äººéƒ½æ¥µå…¶\n",
      "âœ… å¯«å…¥: [æ–°è] å¿«è¨Šï¼å°ç©é›»å˜‰ç¾©å» åˆçˆ†å·¥å®‰äº‹æ•…ã€€å·¥äººé­ã€Œç™¾å…¬æ–¤å†°æ°´ç®¡ã€ç ¸\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] R3hab iverson909 lwamp lovefisk æ”¿æ²»æ–‡ æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: Re: [æ–°è] é»ƒä»å‹³ï¼šè¼•è¦–è¯ç‚ºã€ä¸­åœ‹è£½é€ çš„äººéƒ½æ¥µå…¶\n",
      "âœ… å¯«å…¥: [æ–°è]å°ç©é›»ç¾åœ‹å­¸å¾’è¨ˆç•«ï¼Œæ¼¢å ¡åº—å“¡å·¥è®Šèº«åŠå°\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] longman12579 jengmei æ°´æ¡¶\n",
      "âœ… å¯«å…¥: [æ–°è] å¿«è¨Šï¼å²ä¸Šæœ€å¼·ï¼å°ç©é›»Q2è³º3983å„„å¹´å¢61\n",
      "\n",
      "ğŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index3998.html\n",
      "âœ… å¯«å…¥: [æ–°è] å¾®è»ŸåŸ¹è¨“AIäººæ‰ æ“¬ç ¸40å„„ç¾å…ƒ\n",
      "âœ… å¯«å…¥: [æ–°è] è‹±ç‰¹çˆ¾å‰åŸ·è¡Œé•·å†å‡ºç™¼ï¼Œé–ƒé›»ç¾èº«æ±äº¬ï¼ŒPa\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] ilovetaida æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] encyclopedia æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: ï¼»è«‹ç›Šï¼½offer è«‹ç›Š\n",
      "âœ… å¯«å…¥: Re: [å…¬å‘Š] è·‘æ­¥å“¥åˆ—æœ¬æ¿ä¸å—æ­¡è¿äººç‰©\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] iamala æ”¿æ²»æ–‡ æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [è«‹ç›Š] åŠ›æˆç§‘æŠ€ å®¢æˆ¶å“è³ªå·¥ç¨‹å¸«\n",
      "âœ… å¯«å…¥: Re: [å…¬å‘Š] è·‘æ­¥å“¥åˆ—æœ¬æ¿ä¸å—æ­¡è¿äººç‰©\n",
      "âœ… å¯«å…¥: Re: [å…¬å‘Š] è·‘æ­¥å“¥åˆ—æœ¬æ¿ä¸å—æ­¡è¿äººç‰©\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] D600dust æ°´æ¡¶ä¸‰åæ—¥\n",
      "âœ… å¯«å…¥: [æ–°è] é»ƒä»å‹³ï¼šç¾ç§‘æŠ€å…¬å¸ æŒºå¾—éå·æ™®é—œç¨…\n",
      "âœ… å¯«å…¥: [æ–°è] å¿«è¨Šï¼é«˜é›„ä¸‰å…ƒèƒ½æºç§‘æŠ€å¤§ç«ï¼ã€€14ç”·1å¥³\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] æœ¬ç‰ˆå¾µæ–°ç‰ˆä¸»\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] Dino14124 AreLies æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [è¨è«–] å¤§å®¶æœƒæœ‰æŒ‡å°çé‡‘å—\n",
      "âœ… å¯«å…¥: [æ–°è] ç·¯å‰µå…§æ¹–ç ”ç™¼å¤§æ¨“å‹•å·¥ æ‰“é€ AIç§‘æŠ€å»Šé“æ–°\n",
      "âœ… å¯«å…¥: Re: [æ–°è] æ•æ„Ÿè©é”35è¬ DeepSeekä½¿ç”¨ç‡å¾50%è·Œè‡³3%\n",
      "âœ… å¯«å…¥: [æ–°è]æ—¥æœ¬æ”¿åºœå‚³è¦æ±‚æŒæœ‰æ™¶åœ“ä»£å·¥å»  Rapidusã€Œ\n",
      "âœ… å¯«å…¥: [æ–°è] å°ç©é›»ä¸­ç”Ÿä»£ä¸»ç®¡æ¬é¢¨\n",
      "\n",
      "ğŸ“„ æŠ“å–é é¢: https://www.ptt.cc/bbs/Tech_Job/index3997.html\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] boards ç‰ˆè¦4 æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] a790813a å•†æ¥­å»£å‘Š æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: Fw: [å…¬å‘Š] 7/12 åœæ©Ÿå…¬å‘Š\n",
      "âœ… å¯«å…¥: [è«‹ç›Š] 5å¹´æ¥­å‹™ç¶“é©— ç§‘æŠ€æ¥­å‹™è«‹ç›Šå±¥æ­·èˆ‡è·ç¼º\n",
      "âœ… å¯«å…¥: [è¨è«–] å°ç£å¾®è»Ÿé€™æ¬¡æœ‰äººè¢«è£å“¡å—ï¼Ÿ\n",
      "âœ… å¯«å…¥: [å…¬å‘Š]å¤šé‡åˆ†èº«æ°´æ¡¶\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] å°é”å…¬é—œæª¢èˆ‰å€\n",
      "âœ… å¯«å…¥: [æ–°è] å†é—œé–‰å—ç§‘äº”å» æœ€æ™šæ˜å¹´ä¸­åœç”¢ï¼Ÿç¾¤å‰µï¼šç¨\n",
      "âœ… å¯«å…¥: [è¨è«–] GGå¥½åƒè¦çµ±æ²»å—éƒ¨äº†\n",
      "âœ… å¯«å…¥: [æ–°è] å…¨çƒé¦–å®¶ï¼è¼é”å¸‚å€¼çªç ´4å…†ç¾å…ƒå¤§é—œ\n",
      "âœ… å¯«å…¥: Re: [å…¬å‘Š] å°é”å…¬é—œæª¢èˆ‰å€\n",
      "âœ… å¯«å…¥: Fw: æé†’ä¸€äº›äº‹æƒ…ã€‚\n",
      "âœ… å¯«å…¥: [æ–°è]ã€Œä¿¡ä»»ã€å…©å­—å¤ªæ²‰é‡ï¼ä¸­è¯é›»è‘£åº§åé§ã€Œä¸–\n",
      "âœ… å¯«å…¥: Fw: è«‹æ³¨æ„è‡ªå·±çš„èº«å¿ƒç‹€æ³\n",
      "âœ… å¯«å…¥: Re: [å…¬å‘Š]å¤šé‡åˆ†èº«æ°´æ¡¶\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] è·‘æ­¥å“¥åˆ—æœ¬æ¿ä¸å—æ­¡è¿äººç‰©\n",
      "âœ… å¯«å…¥: [å…¬å‘Š] ctfjuw æ°´æ¡¶ä¸€å€‹æœˆ\n",
      "âœ… å¯«å…¥: Re: [æ–°è] å°ç©é›»æŒç®¡æ¡è³¼å‰¯ç¸½ææ–‡å¦‚ä¼‘é•·å‡\n",
      "âœ… å¯«å…¥: [æ–°è] Googleè–ªè³‡æµå‡ºï¼è»Ÿé«”å·¥ç¨‹å¸«ç ´1000è¬ã€UX\n",
      "âœ… å¯«å…¥: [æ–°è] æ•æ„Ÿè©é”35è¬ DeepSeekä½¿ç”¨ç‡å¾50%è·Œè‡³3%\n",
      "\n",
      "ğŸ‰ å…±è™•ç† 5 é å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def get_latest_index(board_url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(board_url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # å–å¾—ä¸Šä¸€é æŒ‰éˆ•é€£çµ\n",
    "    prev_btn = soup.select_one(\"div.btn-group-paging a.btn.wide:nth-child(2)\")\n",
    "    if prev_btn:\n",
    "        href = prev_btn[\"href\"]  # e.g., /bbs/Tech_Job/index6772.html\n",
    "        match = re.search(r\"index(\\d+)\\.html\", href)\n",
    "        if match:\n",
    "            return int(match.group(1)) + 1  # æœ€æ–°é  = ä¸Šä¸€é  + 1\n",
    "    return None\n",
    "\n",
    "def crawl_page(index):\n",
    "    url = f\"https://www.ptt.cc/bbs/Tech_Job/index{index}.html\"\n",
    "    print(f\"\\nğŸ“„ æŠ“å–é é¢: {url}\")\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    for div in soup.select(\"div.r-ent\"):\n",
    "        title_tag = div.select_one(\"div.title > a\")\n",
    "        date_tag = div.select_one(\"div.meta > div.date\")\n",
    "        nrec_tag = div.select_one(\"div.nrec\")\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            record = {\n",
    "                \"title\": title_tag.text.strip(),\n",
    "                \"date\": date_tag.text.strip(),\n",
    "                \"link\": \"https://www.ptt.cc\" + title_tag[\"href\"],\n",
    "                \"nrec\": nrec_tag.text.strip() if nrec_tag else \"0\"\n",
    "            }\n",
    "            articles.append(record)\n",
    "    return articles\n",
    "\n",
    "def main():\n",
    "    num_pages = 5  # è¦å¾€å‰æŠ“å¹¾é \n",
    "    board_url = \"https://www.ptt.cc/bbs/Tech_Job/index.html\"\n",
    "    latest_index = get_latest_index(board_url)\n",
    "\n",
    "    if not latest_index:\n",
    "        print(\"âŒ ç„¡æ³•å–å¾—æœ€æ–°é ç¢¼\")\n",
    "        return\n",
    "\n",
    "    # é–‹å•Ÿ CSVï¼ˆè¿½åŠ å¯«å…¥ï¼‰\n",
    "    with open(\"ptt_tech_job.csv\", mode='a', encoding='utf-8-sig', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\"title\", \"date\", \"link\", \"nrec\"])\n",
    "        if csv_file.tell() == 0:  # è‹¥ç‚ºæ–°æª”æ¡ˆï¼Œå¯«æ¬„ä½å\n",
    "            writer.writeheader()\n",
    "\n",
    "        # å¾æœ€æ–°é å¾€å‰ N é \n",
    "        for page_index in range(latest_index, latest_index - num_pages, -1):\n",
    "            try:\n",
    "                articles = crawl_page(page_index)\n",
    "                for record in articles:\n",
    "                    writer.writerow(record)\n",
    "                    print(f\"âœ… å¯«å…¥: {record['title']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ç™¼ç”ŸéŒ¯èª¤æ–¼ index{page_index}: {e}\")\n",
    "            \n",
    "            # ç¿»é å¾Œç­‰å¾…\n",
    "            time.sleep(random.uniform(1.0, 2.5))\n",
    "\n",
    "    print(f\"\\nğŸ‰ å…±è™•ç† {num_pages} é å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b6ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ptt_tech_job.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2216b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_recommendations(df, threshold=20):\n",
    "    \"\"\"\n",
    "    Filter posts with recommendations below a threshold\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing posts data\n",
    "        threshold (int): minimum number of recommendations to include\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered DataFrame with nrec < threshold\n",
    "    \"\"\"\n",
    "    # Convert nrec to numeric first\n",
    "    df['nrec'] = pd.to_numeric(df['nrec'].replace('çˆ†', '100').replace('X', '-1'), errors='coerce').fillna(0)\n",
    "    \n",
    "    # Filter and return\n",
    "    return df[df['nrec'] > threshold]\n",
    "\n",
    "def visualize_recommendation_distribution(filtered_df, threshold=20):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of recommendations for filtered data\n",
    "    \n",
    "    Args:\n",
    "        filtered_df (pandas.DataFrame): DataFrame containing filtered posts\n",
    "        threshold (int): Maximum value for visualization bins\n",
    "    \"\"\"\n",
    "    # Display statistics\n",
    "    print(f\"Posts with less than {threshold} recommendations:\")\n",
    "    print(f\"Total count: {len(filtered_df)}\")\n",
    "    print(\"\\nFiltered data preview:\")\n",
    "    print(filtered_df[['title', 'nrec', 'date']].head())\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(filtered_df['nrec'], bins=threshold, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of Posts with < {threshold} Recommendations')\n",
    "    plt.xlabel('Number of Recommendations')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nBasic statistics of filtered recommendations:\")\n",
    "    print(filtered_df['nrec'].describe())\n",
    "\n",
    "# Example usage:\n",
    "threshold = 20\n",
    "filtered_data = filter_low_recommendations(df, threshold)\n",
    "#visualize_recommendation_distribution(filtered_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8926c735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully saved 65 records to ptt_tech_job_below_20_rec.csv\n",
      "ğŸ“ File size: 7.8 KB\n"
     ]
    }
   ],
   "source": [
    "def save_filtered_data(filtered_df, threshold=20, filename=None):\n",
    "    \"\"\"\n",
    "    Save filtered recommendations data to CSV\n",
    "    \n",
    "    Args:\n",
    "        filtered_df (pandas.DataFrame): DataFrame containing filtered posts\n",
    "        threshold (int): Threshold used for filtering (default: 20)\n",
    "        filename (str): Optional custom filename (default: None)\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = f'ptt_tech_job_below_{threshold}_rec.csv'\n",
    "    \n",
    "    try:\n",
    "        # Save to CSV with UTF-8-SIG encoding for Chinese characters\n",
    "        filtered_df.to_csv(filename, encoding='utf-8-sig', index=False)\n",
    "        print(f\"âœ… Successfully saved {len(filtered_df)} records to {filename}\")\n",
    "        \n",
    "        # Show file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # Convert to KB\n",
    "        print(f\"ğŸ“ File size: {file_size:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "\n",
    "save_filtered_data(filtered_data, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902c2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "def get_post_content(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        content_tag = soup.select_one(\"div#main-content\")\n",
    "        if not content_tag:\n",
    "            return \"\"\n",
    "\n",
    "        # ç§»é™¤æ¨æ–‡èˆ‡æ¨™ç±¤\n",
    "        for tag in content_tag.select(\"div, span\"):\n",
    "            tag.extract()\n",
    "\n",
    "        text = content_tag.get_text(separator=\"\\n\").strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æŠ“å–å¤±æ•—: {url} | éŒ¯èª¤: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def crawl_ptt_post_content(filtered_data: pd.DataFrame, output_file: str = \"ptt_tech_job_content.csv\"):\n",
    "    \"\"\"\n",
    "    å¾ PTT å·²ç¯©é¸çš„æ–‡ç« ä¸­æŠ“å–æ¯ç¯‡å…§æ–‡ï¼Œä¸¦å„²å­˜è‡³ CSVï¼Œå…·å‚™é¿å…é‡è¤‡åŠŸèƒ½ã€‚\n",
    "\n",
    "    Args:\n",
    "        filtered_data (pd.DataFrame): å«æœ‰ title/date/link/nrec çš„è³‡æ–™\n",
    "        output_file (str): è¼¸å‡ºæª”æ¡ˆåç¨±\n",
    "    \"\"\"\n",
    "    # === è®€å–å·²å­˜åœ¨çš„ link é¿å…é‡è¤‡ ===\n",
    "    existing_links = set()\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            existing_links = set(existing_df[\"link\"].dropna().tolist())\n",
    "            print(f\"ğŸ“ å·²å­˜åœ¨è³‡æ–™ç­†æ•¸ï¼š{len(existing_links)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ç„¡æ³•è®€å– {output_file}ï¼ŒéŒ¯èª¤ï¼š{e}\")\n",
    "\n",
    "    # === ç¯©é¸å°šæœªè™•ç†çš„é€£çµ ===\n",
    "    to_crawl = filtered_data[~filtered_data[\"link\"].isin(existing_links)]\n",
    "\n",
    "    print(f\"ğŸš€ æº–å‚™æŠ“å–æ–°é€£çµç­†æ•¸ï¼š{len(to_crawl)}\")\n",
    "\n",
    "    # === å¯«å…¥ CSV ===\n",
    "    with open(output_file, mode='a', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"date\", \"link\", \"nrec\", \"content\"])\n",
    "        if f.tell() == 0:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for _, row in to_crawl.iterrows():\n",
    "            url = row[\"link\"]\n",
    "            print(f\"ğŸ” æŠ“å–å…§å®¹: {url}\")\n",
    "            content = get_post_content(url)\n",
    "\n",
    "            writer.writerow({\n",
    "                \"title\": row[\"title\"],\n",
    "                \"date\": row[\"date\"],\n",
    "                \"link\": row[\"link\"],\n",
    "                \"nrec\": row[\"nrec\"],\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "            sleep_time = round(random.uniform(0.2, 1.0), 2)\n",
    "            print(f\"â³ ç­‰å¾… {sleep_time} ç§’...\\n\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    print(\"âœ… å…¨éƒ¨å…§æ–‡æŠ“å–å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ å·²å­˜åœ¨è³‡æ–™ç­†æ•¸ï¼š31\n",
      "ğŸš€ æº–å‚™æŠ“å–æ–°é€£çµç­†æ•¸ï¼š0\n",
      "âœ… å…¨éƒ¨å…§æ–‡æŠ“å–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "crawl_ptt_post_content(filtered_data, output_file=\"ptt_tech_job_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead4b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
