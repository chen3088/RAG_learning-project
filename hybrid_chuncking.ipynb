{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe42bb9",
   "metadata": {},
   "source": [
    "# Hybrid Chunking and FAISS Search\n",
    "1. Import libraries and configuration\n",
    "2. Core functions\n",
    "3. Query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e954fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import annotations\n",
    "import re\n",
    "from typing import List\n",
    "import os\n",
    "from os.path import join as joint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = r\"C:\\Users\\USER\\RAG_learning-project\"\n",
    "BOARD_NAME = \"gossiping\"\n",
    "MODEL_NAME = \"shibing624/text2vec-base-multilingual\"\n",
    "\n",
    "# File paths\n",
    "save_dir = joint(DATA_DIR, BOARD_NAME)\n",
    "index_path = joint(save_dir, \"index_text2vec.faiss\")\n",
    "metadata_path = joint(save_dir, \"metadata_text2vec.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5b5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_dir + f\"/{BOARD_NAME}_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core functions\n",
    "def initial_split(text: str) -> List[str]:\n",
    "    \"\"\"Split text using hybrid rules without discarding separators.\"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"(?:\\n\\s*\\n)+\"      # multiple newlines\n",
    "        r\"|(?=ã€[^ã€‘]+ã€‘)\"      # section headers\n",
    "        r\"|(?=\\d+[\\.\\u3001])\"  # ordered lists like 1. or 1ã€\n",
    "    )\n",
    "    parts = [part.strip() for part in pattern.split(text) if part.strip()]\n",
    "    return parts\n",
    "\n",
    "def build_documents_hybrid_from_df(\n",
    "    df: pd.DataFrame, \n",
    "    chunk_size: int = 300, \n",
    "    chunk_overlap: int = 30\n",
    ") -> List[Document]:\n",
    "    \"\"\"Convert DataFrame rows into Document objects using hybrid chunking.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing a ``content`` column and metadata columns.\n",
    "        chunk_size: Maximum characters per chunk.\n",
    "        chunk_overlap: Overlap size passed to ``RecursiveCharacterTextSplitter``.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: Documents built from all rows of ``df``.\n",
    "    \"\"\"\n",
    "    documents: List[Document] = []\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Convert row to dictionary to ensure proper access\n",
    "        row_dict = row.to_dict()\n",
    "        if \"content\" not in row_dict:\n",
    "            continue\n",
    "            \n",
    "        text = str(row_dict[\"content\"])\n",
    "        # Create metadata excluding content column\n",
    "        metadata = {k: v for k, v in row_dict.items() if k != \"content\"}\n",
    "\n",
    "        segments = initial_split(text)\n",
    "        for segment in segments:\n",
    "            if len(segment) > chunk_size:\n",
    "                sub_segments = splitter.split_text(segment)\n",
    "            else:\n",
    "                sub_segments = [segment]\n",
    "            for sub in sub_segments:\n",
    "                if sub.strip():  # Only add non-empty segments\n",
    "                    documents.append(Document(page_content=sub, metadata=metadata))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def build_faiss_index_from_documents(\n",
    "    documents: List[Document], \n",
    "    save_dir: str,\n",
    "    model_name: str = MODEL_NAME\n",
    ") -> tuple:\n",
    "    \"\"\"Build FAISS index from documents and save to specified directory.\"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    index_path = joint(save_dir, \"index_text2vec.faiss\")\n",
    "    metadata_path = joint(save_dir, \"metadata_text2vec.pkl\")\n",
    "    \n",
    "    # Step 1: Load model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Step 2: Extract texts and metadata\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadata_list = [doc.metadata | {\"text\": doc.page_content} for doc in documents]\n",
    "\n",
    "    # Step 3: Encode\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "    # Step 4: Build and save FAISS\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "    # Step 5: Save metadata\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(metadata_list, f)\n",
    "\n",
    "    return len(documents), index_path, metadata_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = build_documents_hybrid_from_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ae85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b506f93627024a65a06d8dcc7c5d8433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(104,\n",
       " 'C:\\\\Users\\\\USER\\\\RAG_learning-project\\\\gossiping\\\\index_text2vec.faiss',\n",
       " 'C:\\\\Users\\\\USER\\\\RAG_learning-project\\\\gossiping\\\\metadata_text2vec.pkl')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_faiss_index_from_documents(document,save_dir=save_dir)  # documents is a list of Document objects as you showed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b6360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7c235a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded index with 104 vectors\n"
     ]
    }
   ],
   "source": [
    "# FAISS loading functions\n",
    "def load_faiss_components(index_path: str, metadata_path: str) -> tuple:\n",
    "    \"\"\"Load FAISS index and metadata.\"\"\"\n",
    "    try:\n",
    "        # Load FAISS index\n",
    "        index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            metadata_list = pickle.load(f)\n",
    "            \n",
    "        print(f\"Successfully loaded index with {index.ntotal} vectors\")\n",
    "        return index, metadata_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS components: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load FAISS components once\n",
    "FAISS_INDEX, METADATA_LIST = load_faiss_components(index_path, metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcd06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Result 1\n",
      "ğŸ“„ Title: [æ–°è] ä¸¹å¨œçµ²ç½å¾Œ21å¤© å°å—å­¸ç”²å€ä»æŒçºŒæ²’ç¶²è·¯\n",
      "ğŸ“… Date: 7/27\n",
      "ğŸ”— Link: https://www.ptt.cc/bbs/Gossiping/M.1753595102.A.562.html\n",
      "ğŸ“ Text: 1.åª’é«”ä¾†æº:\n",
      "è¯åˆå ± ...\n",
      "\n",
      "ğŸ”¹ Result 2\n",
      "ğŸ“„ Title: [æ–°è] å¤§ç½·å…å¤§æŒ«æ•—æˆ°çŠ¯é‚„åœ¨å¸è²¬ï¼æ¥Šè•™å¦‚ï¼šæ²’æ•‘\n",
      "ğŸ“… Date: 7/27\n",
      "ğŸ”— Link: https://www.ptt.cc/bbs/Gossiping/M.1753630801.A.7AC.html\n",
      "ğŸ“ Text: 8.htm ...\n",
      "\n",
      "ğŸ”¹ Result 3\n",
      "ğŸ“„ Title: [æ–°è] è¬æ·‘è–‡ï¼šè‡ªç”±ä¸æ˜¯æ‹¿ä¾†å‚·äººçš„æ­¦å™¨ ä¸€ç™¼æ–‡\n",
      "ğŸ“… Date: 7/27\n",
      "ğŸ”— Link: https://www.ptt.cc/bbs/Gossiping/M.1753630970.A.44C.html\n",
      "ğŸ“ Text: 8.htm ...\n",
      "\n",
      "ğŸ”¹ Result 4\n",
      "ğŸ“„ Title: [æ–°è] ç½å€ã€Œè‡³ä»Šæ‰‹æ©Ÿé›£é€šè¨Šã€æ²’ç¬¬å››å° ç¶²å‚»çœ¼\n",
      "ğŸ“… Date: 7/28\n",
      "ğŸ”— Link: https://www.ptt.cc/bbs/Gossiping/M.1753635718.A.4B2.html\n",
      "ğŸ“ Text: è®“å—ç½å±…æ°‘æ—©æ—¥æ¢å¾©æ­£å¸¸ç”Ÿæ´»ã€‚\n",
      "ç¶²å‹æ„Ÿå˜†ã€Œåªå› ä½ ä¸æ˜¯éƒ­åœ‹æ–‡å§ã€ã€ã€Œä¸­è¯é›»ä¿¡ä¹Ÿæ˜¯åªæœ‰ä¸€æ ¼è¨Šè™Ÿï¼Œæ²’ç¶²è·¯ã€ã€ã€Œè€å®…ä¸‰\n",
      "æ°‘è·¯ä¹Ÿå¦‚æ­¤ï¼Œæ²’Wi-Fiæ²’ç¬¬å››å°ã€ã€ã€ŒçœŸçˆ›ï¼Œä»€éº¼æ”¿åºœã€ã€‚ ...\n",
      "\n",
      "ğŸ”¹ Result 5\n",
      "ğŸ“„ Title: [æ–°è] ç½å€ã€Œè‡³ä»Šæ‰‹æ©Ÿé›£é€šè¨Šã€æ²’ç¬¬å››å° ç¶²å‚»çœ¼\n",
      "ğŸ“… Date: 7/28\n",
      "ğŸ”— Link: https://www.ptt.cc/bbs/Gossiping/M.1753635718.A.4B2.html\n",
      "ğŸ“ Text: 2.è¨˜è€…ç½²å:\n",
      "ç¥æ½¤éœ–\n",
      "ç½å€ã€Œè‡³ä»Šæ‰‹æ©Ÿé›£é€šè¨Šã€æ²’ç¬¬å››å° ç¶²å‚»çœ¼ï¼šä½ ä¸æ˜¯éƒ­åœ‹æ–‡ ...\n"
     ]
    }
   ],
   "source": [
    "def search_index(\n",
    "    query: str,\n",
    "    index=FAISS_INDEX,\n",
    "    metadata_list=METADATA_LIST,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    top_k: int = 5,\n",
    "    distance_threshold: float = 1.5  # Add threshold to filter irrelevant results\n",
    ") -> List[tuple[dict, float]]:\n",
    "    \"\"\"Search the loaded FAISS index with distance threshold.\"\"\"\n",
    "    # Load model and encode query\n",
    "    model = SentenceTransformer(model_name)\n",
    "    query_vec = model.encode([query]).astype(\"float32\")\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_vec, top_k * 2)  # Get more results initially\n",
    "    \n",
    "    # Get results with distances\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx < len(metadata_list) and dist < distance_threshold:\n",
    "            results.append((metadata_list[idx], float(dist)))\n",
    "    \n",
    "    # Sort by distance and take top_k\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    return results[:top_k]\n",
    "\n",
    "def execute_query(query: str, top_k: int = 5) -> List[dict]:\n",
    "    \"\"\"Execute a query and format results with relevance scores.\"\"\"\n",
    "    results = search_index(query, top_k=top_k)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nğŸ” Query: {query}\")\n",
    "        for i, (res, distance) in enumerate(results, 1):\n",
    "            relevance = max(0, (1 - distance/2)) * 100  # Convert distance to relevance score\n",
    "            print(f\"\\nğŸ”¹ Result {i} (Relevance: {relevance:.1f}%)\")\n",
    "            print(\"ğŸ“„ Title:\", res.get(\"title\"))\n",
    "            print(\"ğŸ“… Date:\", res.get(\"date\"))\n",
    "            print(\"ğŸ”— Link:\", res.get(\"link\"))\n",
    "            print(\"ğŸ“ Text:\", res.get(\"text\")[:200], \"...\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ No relevant results found for query: {query}\")\n",
    "    \n",
    "    return [r[0] for r in results]  # Return just the metadata dictionaries\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"é¢±é¢¨é€ æˆä»€éº¼ç½æƒ…ï¼Ÿ\",\n",
    "    \"å°å—æœ‰ä»€éº¼ç‹€æ³ï¼Ÿ\",\n",
    "    \"é€šè¨Šä¸­æ–·çš„æƒ…æ³ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    results = execute_query(query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91592d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
