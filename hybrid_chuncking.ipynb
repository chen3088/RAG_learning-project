{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe42bb9",
   "metadata": {},
   "source": [
    "# Hybrid Chunking and FAISS Search\n",
    "1. Import libraries and configuration\n",
    "2. Core functions\n",
    "3. Query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e954fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import annotations\n",
    "import re\n",
    "from typing import List\n",
    "import os\n",
    "from os.path import join as joint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = r\"C:\\Users\\USER\\RAG_learning-project\"\n",
    "BOARD_NAME = \"gossiping\"\n",
    "MODEL_NAME = \"shibing624/text2vec-base-multilingual\"\n",
    "\n",
    "# File paths\n",
    "save_dir = joint(DATA_DIR, BOARD_NAME)\n",
    "index_path = joint(save_dir, \"index_text2vec.faiss\")\n",
    "metadata_path = joint(save_dir, \"metadata_text2vec.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5b5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_dir + f\"/{BOARD_NAME}_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core functions\n",
    "def initial_split(text: str) -> List[str]:\n",
    "    \"\"\"Split text using hybrid rules without discarding separators.\"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"(?:\\n\\s*\\n)+\"      # multiple newlines\n",
    "        r\"|(?=【[^】]+】)\"      # section headers\n",
    "        r\"|(?=\\d+[\\.\\u3001])\"  # ordered lists like 1. or 1、\n",
    "    )\n",
    "    parts = [part.strip() for part in pattern.split(text) if part.strip()]\n",
    "    return parts\n",
    "\n",
    "def build_documents_hybrid_from_df(\n",
    "    df: pd.DataFrame, \n",
    "    chunk_size: int = 300, \n",
    "    chunk_overlap: int = 30\n",
    ") -> List[Document]:\n",
    "    \"\"\"Convert DataFrame rows into Document objects using hybrid chunking.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing a ``content`` column and metadata columns.\n",
    "        chunk_size: Maximum characters per chunk.\n",
    "        chunk_overlap: Overlap size passed to ``RecursiveCharacterTextSplitter``.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: Documents built from all rows of ``df``.\n",
    "    \"\"\"\n",
    "    documents: List[Document] = []\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Convert row to dictionary to ensure proper access\n",
    "        row_dict = row.to_dict()\n",
    "        if \"content\" not in row_dict:\n",
    "            continue\n",
    "            \n",
    "        text = str(row_dict[\"content\"])\n",
    "        # Create metadata excluding content column\n",
    "        metadata = {k: v for k, v in row_dict.items() if k != \"content\"}\n",
    "\n",
    "        segments = initial_split(text)\n",
    "        for segment in segments:\n",
    "            if len(segment) > chunk_size:\n",
    "                sub_segments = splitter.split_text(segment)\n",
    "            else:\n",
    "                sub_segments = [segment]\n",
    "            for sub in sub_segments:\n",
    "                if sub.strip():  # Only add non-empty segments\n",
    "                    documents.append(Document(page_content=sub, metadata=metadata))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def build_faiss_index_from_documents(\n",
    "    documents: List[Document], \n",
    "    save_dir: str,\n",
    "    model_name: str = MODEL_NAME\n",
    ") -> tuple:\n",
    "    \"\"\"Build FAISS index from documents and save to specified directory.\"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    index_path = joint(save_dir, \"index_text2vec.faiss\")\n",
    "    metadata_path = joint(save_dir, \"metadata_text2vec.pkl\")\n",
    "    \n",
    "    # Step 1: Load model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Step 2: Extract texts and metadata\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadata_list = [doc.metadata | {\"text\": doc.page_content} for doc in documents]\n",
    "\n",
    "    # Step 3: Encode\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "    # Step 4: Build and save FAISS\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "    # Step 5: Save metadata\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(metadata_list, f)\n",
    "\n",
    "    return len(documents), index_path, metadata_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = build_documents_hybrid_from_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ae85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b506f93627024a65a06d8dcc7c5d8433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(104,\n",
       " 'C:\\\\Users\\\\USER\\\\RAG_learning-project\\\\gossiping\\\\index_text2vec.faiss',\n",
       " 'C:\\\\Users\\\\USER\\\\RAG_learning-project\\\\gossiping\\\\metadata_text2vec.pkl')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_faiss_index_from_documents(document,save_dir=save_dir)  # documents is a list of Document objects as you showed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b6360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7c235a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded index with 104 vectors\n"
     ]
    }
   ],
   "source": [
    "# FAISS loading functions\n",
    "def load_faiss_components(index_path: str, metadata_path: str) -> tuple:\n",
    "    \"\"\"Load FAISS index and metadata.\"\"\"\n",
    "    try:\n",
    "        # Load FAISS index\n",
    "        index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            metadata_list = pickle.load(f)\n",
    "            \n",
    "        print(f\"Successfully loaded index with {index.ntotal} vectors\")\n",
    "        return index, metadata_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS components: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load FAISS components once\n",
    "FAISS_INDEX, METADATA_LIST = load_faiss_components(index_path, metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcd06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Result 1\n",
      "📄 Title: [新聞] 丹娜絲災後21天 台南學甲區仍持續沒網路\n",
      "📅 Date: 7/27\n",
      "🔗 Link: https://www.ptt.cc/bbs/Gossiping/M.1753595102.A.562.html\n",
      "📝 Text: 1.媒體來源:\n",
      "聯合報 ...\n",
      "\n",
      "🔹 Result 2\n",
      "📄 Title: [新聞] 大罷免大挫敗戰犯還在卸責！楊蕙如：沒救\n",
      "📅 Date: 7/27\n",
      "🔗 Link: https://www.ptt.cc/bbs/Gossiping/M.1753630801.A.7AC.html\n",
      "📝 Text: 8.htm ...\n",
      "\n",
      "🔹 Result 3\n",
      "📄 Title: [新聞] 謝淑薇：自由不是拿來傷人的武器 一發文\n",
      "📅 Date: 7/27\n",
      "🔗 Link: https://www.ptt.cc/bbs/Gossiping/M.1753630970.A.44C.html\n",
      "📝 Text: 8.htm ...\n",
      "\n",
      "🔹 Result 4\n",
      "📄 Title: [新聞] 災區「至今手機難通訊」沒第四台 網傻眼\n",
      "📅 Date: 7/28\n",
      "🔗 Link: https://www.ptt.cc/bbs/Gossiping/M.1753635718.A.4B2.html\n",
      "📝 Text: 讓受災居民早日恢復正常生活。\n",
      "網友感嘆「只因你不是郭國文吧」、「中華電信也是只有一格訊號，沒網路」、「老宅三\n",
      "民路也如此，沒Wi-Fi沒第四台」、「真爛，什麼政府」。 ...\n",
      "\n",
      "🔹 Result 5\n",
      "📄 Title: [新聞] 災區「至今手機難通訊」沒第四台 網傻眼\n",
      "📅 Date: 7/28\n",
      "🔗 Link: https://www.ptt.cc/bbs/Gossiping/M.1753635718.A.4B2.html\n",
      "📝 Text: 2.記者署名:\n",
      "祝潤霖\n",
      "災區「至今手機難通訊」沒第四台 網傻眼：你不是郭國文 ...\n"
     ]
    }
   ],
   "source": [
    "def search_index(\n",
    "    query: str,\n",
    "    index=FAISS_INDEX,\n",
    "    metadata_list=METADATA_LIST,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    top_k: int = 5,\n",
    "    distance_threshold: float = 1.5  # Add threshold to filter irrelevant results\n",
    ") -> List[tuple[dict, float]]:\n",
    "    \"\"\"Search the loaded FAISS index with distance threshold.\"\"\"\n",
    "    # Load model and encode query\n",
    "    model = SentenceTransformer(model_name)\n",
    "    query_vec = model.encode([query]).astype(\"float32\")\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_vec, top_k * 2)  # Get more results initially\n",
    "    \n",
    "    # Get results with distances\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx < len(metadata_list) and dist < distance_threshold:\n",
    "            results.append((metadata_list[idx], float(dist)))\n",
    "    \n",
    "    # Sort by distance and take top_k\n",
    "    results.sort(key=lambda x: x[1])\n",
    "    return results[:top_k]\n",
    "\n",
    "def execute_query(query: str, top_k: int = 5) -> List[dict]:\n",
    "    \"\"\"Execute a query and format results with relevance scores.\"\"\"\n",
    "    results = search_index(query, top_k=top_k)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n🔍 Query: {query}\")\n",
    "        for i, (res, distance) in enumerate(results, 1):\n",
    "            relevance = max(0, (1 - distance/2)) * 100  # Convert distance to relevance score\n",
    "            print(f\"\\n🔹 Result {i} (Relevance: {relevance:.1f}%)\")\n",
    "            print(\"📄 Title:\", res.get(\"title\"))\n",
    "            print(\"📅 Date:\", res.get(\"date\"))\n",
    "            print(\"🔗 Link:\", res.get(\"link\"))\n",
    "            print(\"📝 Text:\", res.get(\"text\")[:200], \"...\")\n",
    "    else:\n",
    "        print(f\"\\n❌ No relevant results found for query: {query}\")\n",
    "    \n",
    "    return [r[0] for r in results]  # Return just the metadata dictionaries\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"颱風造成什麼災情？\",\n",
    "    \"台南有什麼狀況？\",\n",
    "    \"通訊中斷的情況？\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    results = execute_query(query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91592d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
